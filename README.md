# AI Model Extraction Simulator
## Makine Ã–ÄŸrenimi Model Ã‡alma SimÃ¼latÃ¶rÃ¼

**EÄŸitim ve gÃ¼venlik araÅŸtÄ±rmasÄ± iÃ§in** geliÅŸtirilmiÅŸ kapsamlÄ± model Ã§Ä±karma simÃ¼latÃ¶rÃ¼.

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

**âš¡ 5 dakikada baÅŸlamak istiyorsanÄ±z**: [`QUICK_START.md`](QUICK_START.md) dosyasÄ±na bakÄ±n!

### ğŸ”’ GÃ¼venli Mod (Ã–nerilen):
```bash
cd simulation/
python simulator.py
```

### âš ï¸ Real-World Mod (Ä°zin Gerekli):
```bash
cd real_world/
python real_world_attack.py --analyze-only
```

---

## ğŸ“– Ä°Ã§indekiler

- [ğŸ¯ Ne Yapar Bu Simulator?](#-ne-yapar-bu-simulator)
- [ï¿½ Proje YapÄ±sÄ±](#-proje-yapÄ±sÄ±)
- [ğŸ”§ Kurulum](#-kurulum)
- [ï¿½ğŸ”’ GÃ¼venli EÄŸitim ModÃ¼lÃ¼](#-mod-1-gÃ¼venli-eÄŸitim-simÃ¼latÃ¶rÃ¼-Ã¶nerilen)
- [âš ï¸ Real-World Attack ModÃ¼lÃ¼](#ï¸-mod-2-real-world-api-saldÄ±rÄ±larÄ±)
- [ï¿½ KarÅŸÄ±laÅŸtÄ±rma Tablosu](#-karÅŸÄ±laÅŸtÄ±rma-tablosu)
- [ğŸ”§ Teknik Ã–zellikler](#-teknik-Ã¶zellikler)
- [ğŸ”§ Troubleshooting](#-troubleshooting)
- [ğŸ“š EÄŸitim AmaÃ§larÄ±](#-eÄŸitim-amaÃ§larÄ±)

---

## ğŸ¯ Ne Yapar Bu Simulator?

Bu proje, **AI model extraction** saldÄ±rÄ±larÄ±nÄ± simÃ¼le eder:

1. **ğŸ¯ Target Model**: Hedef AI modelini taklit eder
2. **ğŸ•µï¸ Attack Strategies**: FarklÄ± saldÄ±rÄ± tekniklerini uygular  
3. **ğŸ¤– Clone Training**: Ã‡alÄ±nan verilerle klon model eÄŸitir
4. **ğŸ“Š Success Evaluation**: SaldÄ±rÄ±nÄ±n baÅŸarÄ±sÄ±nÄ± Ã¶lÃ§er

### Desteklenen Teknikler:
- **Query Strategies**: Random, Active Learning, Adversarial
- **Model Architectures**: ResNet, VGG, Custom CNNs
- **Knowledge Distillation**: Temperature-based training
- **Real-World APIs**: Google Vision, AWS Rekognition, Custom APIs

---

## ğŸ“ Proje YapÄ±sÄ±

```
model-stealer/
â”œâ”€â”€ src/                          # Core kaynak kodlarÄ±
â”‚   â”œâ”€â”€ victim_model/            # Hedef model simÃ¼lasyonu
â”‚   â”œâ”€â”€ attacker/                # SaldÄ±rÄ± stratejileri
â”‚   â”œâ”€â”€ clone_model/             # Klon model eÄŸitimi
â”‚   â””â”€â”€ utils/                   # YardÄ±mcÄ± araÃ§lar
â”œâ”€â”€ simulation/                   # ğŸ”’ GÃ¼venli eÄŸitim ortamÄ±
â”‚   â”œâ”€â”€ config/                  # SimÃ¼lasyon ayarlarÄ±
â”‚   â”œâ”€â”€ simulator.py             # Ana simÃ¼lasyon scripti
â”‚   â””â”€â”€ README.md                # SimÃ¼lasyon rehberi
â”œâ”€â”€ real_world/                   # âš ï¸ GerÃ§ek API saldÄ±rÄ±larÄ± (Ä°ZÄ°NLÄ°)
â”‚   â”œâ”€â”€ config/                  # Real-world ayarlarÄ±
â”‚   â”œâ”€â”€ adapters/                # API adaptÃ¶rleri
â”‚   â”œâ”€â”€ examples/                # API-specific Ã¶rnekler
â”‚   â”œâ”€â”€ real_world_attack.py     # Ana saldÄ±rÄ± scripti
â”‚   â””â”€â”€ README.md                # Real-world rehberi
â”œâ”€â”€ examples/                     # Ã–rnek kullanÄ±mlar
â”œâ”€â”€ docs/                        # DokÃ¼mantasyon
â”œâ”€â”€ tests/                       # Test dosyalarÄ±
â””â”€â”€ requirements.txt             # Gerekli paketler
```

---

## ğŸ”§ Kurulum

```bash
# Projeyi klonla
git clone https://github.com/ertanszr-git/AI-Model-Stealer-Simulation.git
cd AI-Model-Stealer-Simulation

# Otomatik kurulum
chmod +x setup.sh
./setup.sh

# Manuel kurulum
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

---

## ğŸ”’ Mod 1: GÃ¼venli EÄŸitim SimÃ¼latÃ¶rÃ¼ (Ã–nerilen)

### ğŸ“š Ne Ä°Ã§in KullanÄ±lÄ±r?
- âœ… **EÄŸitim amaÃ§lÄ±**: Model extraction tekniklerini Ã¶ÄŸrenmek
- âœ… **GÃ¼venli test**: HiÃ§bir yasal risk olmadan deneme
- âœ… **AraÅŸtÄ±rma**: Algorithm geliÅŸtirme ve test etme
- âœ… **Demo**: TekniÄŸi gÃ¶sterme ve anlatma

### ğŸš€ NasÄ±l KullanÄ±lÄ±r?

#### Basit KullanÄ±m:
```bash
# SimÃ¼lasyon klasÃ¶rÃ¼ne git
cd simulation/

# VarsayÄ±lan ayarlarla Ã§alÄ±ÅŸtÄ±r
python simulator.py
```

#### KonfigÃ¼rasyon ile:
```bash
# Kendi ayarlarÄ±nÄ±zla Ã§alÄ±ÅŸtÄ±r
python simulator.py --config config/simulation.yaml
```

#### AdÄ±m AdÄ±m Ã–rnek:
```bash
# 1. Sanal ortamÄ± aktive et
source venv/bin/activate  # macOS/Linux

# 2. SimÃ¼lasyon klasÃ¶rÃ¼ne git
cd simulation/

# 3. FarklÄ± stratejilerle deneyin
python simulator.py  # VarsayÄ±lan: ResNet18 vs ResNet18

# 4. SonuÃ§larÄ± inceleyin
ls -la ../experiments/simulation_*/
```

### âš™ï¸ KonfigÃ¼rasyon SeÃ§enekleri:

`simulation/config/simulation.yaml` dosyasÄ±nÄ± dÃ¼zenleyerek:

```yaml
# Hedef model deÄŸiÅŸtir
victim_model:
  type: "vgg16"        # resnet18, resnet50, vgg16
  dataset: "cifar100"  # cifar10, cifar100, imagenet
  num_classes: 100

# SaldÄ±rÄ± stratejisi seÃ§
attack:
  strategy: "active_learning"  # random, active_learning, adversarial
  query_budget: 5000          # KaÃ§ sorgu yapÄ±lacak
  batch_size: 64              # Batch boyutu

# Klon model mimarisi
clone_model:
  architecture: "lightweight_cnn"  # FarklÄ± mimari dene
  training_epochs: 100              # Daha uzun eÄŸitim
```

### ğŸ“Š Beklenen Ã‡Ä±ktÄ±lar:
```
ğŸ“ experiments/simulation_baseline_extraction_2024_07_24_10_30/
â”œâ”€â”€ ğŸ“Š results/
â”‚   â””â”€â”€ experiment_results.json    # Fidelity: %95.2, Queries: 10000
â”œâ”€â”€ ğŸ¤– models/
â”‚   â””â”€â”€ clone_model.pth           # EÄŸitilmiÅŸ klon model
â”œâ”€â”€ ğŸ“ˆ logs/
â”‚   â””â”€â”€ simulation.log            # DetaylÄ± iÅŸlem loglarÄ±
â””â”€â”€ ğŸ’¾ data/
    â”œâ”€â”€ stolen_queries.npy        # Ã‡alÄ±nan sorgu verileri
    â””â”€â”€ stolen_responses.npy      # API yanÄ±tlarÄ±
```

---

## âš ï¸ Mod 2: Real-World API SaldÄ±rÄ±larÄ±

### ğŸš¨ YASAL UYARI
**Bu mod sadece izniniz olan API'lerde kullanÄ±labilir!**
- âš–ï¸ Ä°zinsiz kullanÄ±m **yasadÄ±ÅŸÄ±dÄ±r**
- ğŸ’¼ Yasal sorumluluk **tamamen sizde**
- ğŸ“ **YazÄ±lÄ± izin** almalÄ±sÄ±nÄ±z

### ğŸ¯ Ne Ä°Ã§in KullanÄ±lÄ±r?
- ğŸ”¬ **GÃ¼venlik araÅŸtÄ±rmasÄ±**: Bug bounty programlarÄ±
- ğŸ›¡ï¸ **Kendi API'nizi test**: Savunma geliÅŸtirme
- ğŸ“ **Akademik Ã§alÄ±ÅŸma**: Ä°zinli araÅŸtÄ±rma projeleri
- ğŸ” **Penetrasyon testi**: Red team egzersizleri

### ğŸ“‹ Ã–nce YapmanÄ±z Gerekenler:

#### 1. Yasal Ä°zin AlÄ±n:
```markdown
â˜ API sahibinden yazÄ±lÄ± izin aldÄ±m
â˜ Terms of Service'i okudum
â˜ Test kapsamÄ±nÄ± belirledim  
â˜ Hukuki danÄ±ÅŸmanlÄ±k aldÄ±m
â˜ Sorumlu aÃ§Ä±klama planÄ±m hazÄ±r
```

#### 2. API Bilgilerini HazÄ±rlayÄ±n:
```yaml
# real_world/config/real_world.yaml
target_api:
  endpoint: "https://your-api.com/predict"
  api_key: "your-api-key"
  format: "json"  # veya "google_vision", "aws_rekognition"
```

### ğŸš€ AdÄ±m AdÄ±m KullanÄ±m:

#### AdÄ±m 1: GÃ¼venli Analiz
```bash
cd real_world/

# API'yi analiz et (saldÄ±rÄ± yok, sadece keÅŸif)
python real_world_attack.py --analyze-only
```

**Ã‡Ä±ktÄ± Ã¶rneÄŸi:**
```
ğŸ” Target Analysis Results:
- Rate limits: 1000/hour
- Input format: base64 images
- Response format: JSON with predictions
- Estimated cost: $0.001 per request
```

#### AdÄ±m 2: Test KonfigÃ¼rasyonu
```bash
# KonfigÃ¼rasyonu test et
python real_world_attack.py --config config/real_world.yaml --analyze-only
```

#### AdÄ±m 3: KÃ¼Ã§Ã¼k Pilot Test
```yaml
# Ã–nce kÃ¼Ã§Ã¼k budget ile test
attack:
  query_budget: 100    # Sadece 100 sorgu
  batch_size: 1        # Tek tek sorgu
  strategy: "random"   # Basit strateji
```

#### AdÄ±m 4: Tam SaldÄ±rÄ± (Ä°zin gerekli!)
```bash
# GerÃ§ek saldÄ±rÄ± - DÄ°KKATLÄ°!
python real_world_attack.py --config config/real_world.yaml
```

### ğŸ¯ Desteklenen API'ler:

#### Google Cloud Vision API:
```yaml
target_api:
  endpoint: "https://vision.googleapis.com/v1/images:annotate"
  api_key: "your-google-api-key"
  format: "google_vision"
  cost_per_request: 0.0015
```

#### AWS Rekognition:
```yaml
target_api:
  format: "aws_rekognition" 
  aws_access_key: "AKIA..."
  aws_secret_key: "secret..."
  region: "us-east-1"
  cost_per_request: 0.001
```

#### Custom REST API:
```yaml
target_api:
  endpoint: "https://my-api.com/classify"
  headers:
    "Authorization": "Bearer token"
    "Content-Type": "application/json"
  format: "json"
  input_format: "base64"
```

### ğŸ“Š Ã–rnek Real-World SonuÃ§larÄ±:
```
ğŸ¯ Google Vision API Attack Results:
- Fidelity: 87.3%
- Queries used: 2,847/5,000
- Total cost: $4.27
- Success rate: 94.2%
- Attack duration: 2h 15m

âš ï¸ Recommendations:
- Model shows vulnerability to extraction
- Consider implementing query budgets
- Add differential privacy noise
```

### ğŸ›¡ï¸ GÃ¼venlik Ã–zellikleri:

#### Rate Limiting:
```yaml
# Etik kullanÄ±m iÃ§in
rate_limit_delay: 2.0      # 2 saniye bekleme
randomize_delays: true     # Rastgele gecikme
max_total_cost: 50.0       # Maksimum $50
```

#### Stealth Mode:
```yaml
stealth:
  user_agent_rotation: true
  temporal_distribution: true
  request_randomization: true
```

---

## ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma Tablosu

| Ã–zellik | ğŸ”’ SimÃ¼lasyon | âš ï¸ Real-World |
|---------|---------------|---------------|
| **GÃ¼venlik** | âœ… Tamamen gÃ¼venli | ğŸš¨ Yasal risk var |
| **Ã–ÄŸrenim** | âœ… MÃ¼kemmel | âš ï¸ Dikkatli |
| **Maliyet** | âœ… Ãœcretsiz | ğŸ’° API maliyeti |
| **HÄ±z** | âœ… Ã‡ok hÄ±zlÄ± | â±ï¸ Rate limited |
| **GerÃ§eklik** | âš ï¸ SimÃ¼lasyon | âœ… GerÃ§ek veriler |
| **Ä°zin** | âœ… Gerek yok | ğŸš¨ Mutlaka gerekli |

## ğŸ† Hangi Modu SeÃ§meli?

### ğŸ“ Ã–ÄŸrenim/EÄŸitim iÃ§in:
**â¡ï¸ SimÃ¼lasyon modunu seÃ§in**
- HiÃ§bir risk yok
- HÄ±zlÄ± sonuÃ§
- SÄ±nÄ±rsÄ±z deneme

### ğŸ”¬ AraÅŸtÄ±rma iÃ§in:
**â¡ï¸ Ã–nce simÃ¼lasyon, sonra real-world**
1. SimÃ¼lasyonda algoritmanÄ±zÄ± geliÅŸtirin
2. Yasal izin alÄ±n  
3. Real-world'de test edin

### ğŸ›¡ï¸ GÃ¼venlik testi iÃ§in:
**â¡ï¸ Real-world (kendi API'nÄ±z)**
- Kendi API'nizi test edin
- GerÃ§ek vulnerabilityler keÅŸfedin
- Savunma geliÅŸtirin

---

```bash
# Projeyi klonla
git clone https://github.com/ertanszr-git/AI-Model-Stealer-Simulation.git
cd AI-Model-Stealer-Simulation

# Otomatik kurulum
chmod +x setup.sh
./setup.sh

# Manuel kurulum
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

---

## ğŸ”§ Teknik Ã–zellikler

### âœ… Tamamlanan Ã–zellikler

- **Kurban Model SimÃ¼lasyonu**: ResNet, VGG gibi modern mimariler
- **SaldÄ±rÄ± Stratejileri**: 
  - Rastgele sorgulama
  - Aktif Ã¶ÄŸrenme tabanlÄ±
  - Adversarial sorgulama
- **Klon Model EÄŸitimi**: Knowledge Distillation ile
- **DeÄŸerlendirme Metrikleri**:
  - Model Fidelity (Uyum oranÄ±)
  - Query Efficiency (Sorgu verimliliÄŸi)
  - Cosine Similarity
- **Savunma MekanizmalarÄ±**:
  - Rate limiting
  - Output noise injection
  - Query monitoring

### ğŸ”§ Teknik Detaylar

**Desteklenen Mimariler:**
- ResNet (18, 50), VGG16, Custom CNN modeller

**Veri Setleri:**
- CIFAR-10/100, Custom datasets

**SaldÄ±rÄ± Stratejileri:**
- Random queries, Active learning, Adversarial queries

## ğŸ“Š Ã–rnek SonuÃ§lar

```
ğŸ“Š Experiment Summary:
Victim Model Accuracy: 0.9234
Clone Model Accuracy: 0.8567
Model Fidelity: 0.8934
Queries Used: 2,000
Query Efficiency: 4.467 (fidelity per 1000 queries)
```

## ğŸ›¡ï¸ GÃ¼venlik ve Savunma

Proje ayrÄ±ca ÅŸu savunma mekanizmalarÄ±nÄ± simÃ¼le eder:

1. **API Rate Limiting**: Sorgu hÄ±zÄ± sÄ±nÄ±rlama
2. **Output Perturbation**: Ã‡Ä±ktÄ±lara noise ekleme
3. **Query Pattern Detection**: Anormal sorgu tespiti
4. **Differential Privacy**: Gizlilik koruyucu teknikler

## ğŸ“Š Performans Beklentileri

### ğŸ”’ SimÃ¼lasyon Modu:
- **HÄ±z**: 1000-5000 sorgu/dakika
- **Fidelity**: %85-98 (tipik)
- **SÃ¼re**: 5-30 dakika
- **Maliyet**: $0

### âš ï¸ Real-World Modu:
- **HÄ±z**: 10-100 sorgu/dakika (rate limit)
- **Fidelity**: %70-95 (API'ye gÃ¶re)
- **SÃ¼re**: 1-10 saat
- **Maliyet**: $1-100 (budget'a gÃ¶re)

## ğŸ”§ Troubleshooting

### âŒ SÄ±k KarÅŸÄ±laÅŸÄ±lan Sorunlar:

#### 1. Import HatalarÄ±:
```bash
# Ã‡Ã¶zÃ¼m: Virtual environment aktive edin
source venv/bin/activate
pip install -r requirements.txt
```

#### 2. CUDA/MPS HatalarÄ±:
```bash
# Ã‡Ã¶zÃ¼m: Config'de device ayarÄ±
device: "cpu"  # veya "mps" (Mac), "cuda" (GPU)
```

#### 3. API Connection Failed:
```bash
# Ã‡Ã¶zÃ¼m: API key ve endpoint kontrol
python real_world_attack.py --analyze-only  # Test connection
```

#### 4. Rate Limit Exceeded:
```yaml
# Ã‡Ã¶zÃ¼m: Config'de rate limit artÄ±r
rate_limit_delay: 5.0  # 5 saniye bekleme
batch_size: 1          # Tek sorgu
```

#### 5. Low Fidelity Results:
```yaml
# Ã‡Ã¶zÃ¼m: Daha fazla sorgu veya better strategy
attack:
  query_budget: 10000
  strategy: "active_learning"
  temperature: 3.0
```

#### 6. Memory Issues:
```yaml
# Ã‡Ã¶zÃ¼m: Batch size'Ä± azalt
clone_model:
  batch_size: 16  # VarsayÄ±lan 64'ten dÃ¼ÅŸÃ¼r
```

### ğŸ†˜ Destek Almak:

1. **Teknik Sorunlar**: GitHub Issues
2. **Yasal Sorular**: Hukuk mÃ¼ÅŸaviri  
3. **Etik Konular**: AraÅŸtÄ±rma topluluÄŸu
4. **API Problems**: Ä°lgili API dokÃ¼mantasyonu

## ğŸ“ˆ Ä°leri Seviye KullanÄ±m

### Custom Strategy GeliÅŸtirme:
```python
# src/attacker/custom_strategy.py
from .extraction_strategies import QueryStrategy

class MyCustomStrategy(QueryStrategy):
    def select_queries(self, pool, budget):
        # Kendi algoritmanÄ±z
        return selected_queries
```

### Multi-Target Attacks:
```bash
# Birden fazla hedef test et
for api in google_vision aws_rekognition azure_vision; do
    python real_world_attack.py --config config/${api}.yaml
done
```

### Ensemble Attacks:
```yaml
# Birden fazla stratejinin kombinasyonu
attack:
  strategy: "ensemble"
  strategies: ["random", "active_learning", "adversarial"]
  weights: [0.3, 0.5, 0.2]
```

## ğŸ“š EÄŸitim AmaÃ§larÄ±

Bu simÃ¼latÃ¶r ÅŸunlarÄ± Ã¶ÄŸrenmeye yardÄ±mcÄ± olur:

- Model extraction saldÄ±rÄ±larÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±
- FarklÄ± sorgulama stratejilerinin etkinliÄŸi
- ML sistemlerinde API gÃ¼venliÄŸinin Ã¶nemi
- Model theft'e karÅŸÄ± savunma yÃ¶ntemleri
- Knowledge distillation ve transfer learning

## ğŸ¤ KatkÄ±da Bulunma

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ Lisans

Bu proje eÄŸitim amaÃ§lÄ±dÄ±r. LÃ¼tfen SECURITY.md dosyasÄ±nÄ± okuyun.

## ğŸ“ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in issue aÃ§abilir veya [contact] ile iletiÅŸime geÃ§ebilirsiniz.

---

**âš ï¸ HatÄ±rlatma**: Bu araÃ§ yalnÄ±zca eÄŸitim ve araÅŸtÄ±rma amaÃ§lÄ±dÄ±r. Etik kurallara uygun kullanÄ±n!
